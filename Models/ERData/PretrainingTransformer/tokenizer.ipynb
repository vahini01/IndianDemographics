{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d555a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers.trainers import WordPieceTrainer, BpeTrainer, WordLevelTrainer\n",
    "from tokenizers import AddedToken, Tokenizer\n",
    "from tokenizers.models import WordPiece, BPE, WordLevel\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.pre_tokenizers import Whitespace, WhitespaceSplit, Split\n",
    "from tokenizers.normalizers import NFD, Lowercase\n",
    "from tokenizers.processors import BertProcessing, RobertaProcessing\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8564c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/souvic/mounted/btp/vahini/Name2Demographics\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_FOLDER = \"../../../\"\n",
    "%cd $PATH_TO_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e0d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "\n",
    "sys.path.insert(0, 'Models/ERData/PreProcessing/')\n",
    "from er_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55412ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "er = ERData_with_dup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42791543",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = er['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68468525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22217062"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d944f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bert word piece tokenizer\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD()])\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=10000)\n",
    "tokenizer.train_from_iterator(names, trainer=trainer)\n",
    "tokenizer.post_processor = BertProcessing(sep=(\"[SEP]\", tokenizer.get_vocab()['[SEP]']), cls=(\"[CLS]\", tokenizer.get_vocab()['[CLS]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5413b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b303d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Models/ERData/Tokenizer/trained_tokenizer/indiannames-tokenizer/tokenizer_config.json',\n",
       " 'Models/ERData/Tokenizer/trained_tokenizer/indiannames-tokenizer/special_tokens_map.json',\n",
       " 'Models/ERData/Tokenizer/trained_tokenizer/indiannames-tokenizer/vocab.txt',\n",
       " 'Models/ERData/Tokenizer/trained_tokenizer/indiannames-tokenizer/added_tokens.json',\n",
       " 'Models/ERData/Tokenizer/trained_tokenizer/indiannames-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.save_pretrained('Models/ERData/Tokenizer/'+\"trained_tokenizer/indiannames-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689d2ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'sk', 'shel', '##ley', 'lamkang', 'sankh', '##il', '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"sk shelley lamkang sankhil\")\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5daee4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Character level tokenization\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD()])\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=500)\n",
    "tokenizer.train_from_iterator(names, trainer=trainer)\n",
    "tokenizer.post_processor = BertProcessing(sep=(\"[SEP]\", tokenizer.get_vocab()['[SEP]']), cls=(\"[CLS]\", tokenizer.get_vocab()['[CLS]']))\n",
    "\n",
    "tokenizer.get_vocab()\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a11d78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Models/ERData/Tokenizer/trained_tokenizer/indiannames-char-tokenizer/tokenizer_config.json',\n",
       " 'Models/ERData/Tokenizer/trained_tokenizer/indiannames-char-tokenizer/special_tokens_map.json',\n",
       " 'Models/ERData/Tokenizer/trained_tokenizer/indiannames-char-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.convert_ids_to_tokens(fast_tokenizer(\"prasanna PARASURAMA\")['input_ids'])\n",
    "\n",
    "fast_tokenizer.save_pretrained('Models/ERData/Tokenizer/'+\"trained_tokenizer/indiannames-char-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ce4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "muriltokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "\n",
    "berttokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24302bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "muriltokenizer.tokenize(\"sk shelley lamkang sankhil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c371c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "berttokenizer.tokenize(\"sk shelley lamkang sankhil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31fac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"sk shelley lamkang sankhil\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ad1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py38] *",
   "language": "python",
   "name": "conda-env-.conda-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
